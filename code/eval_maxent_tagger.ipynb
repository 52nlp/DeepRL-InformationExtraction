{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy.sparse\n",
    "import time\n",
    "import itertools\n",
    "import sys\n",
    "import pickle\n",
    "import helper\n",
    "\n",
    "\n",
    "#tags2int = {\"TAG\": 0, \"shooterName\":1, \"killedNum\":2, \"woundedNum\":3, \"city\":4}\n",
    "# tags2int={'TAG':0, 'food':1, 'adulterant':2, 'location':3, 'year':4}\n",
    "# #int2tags = [\"TAG\",'shooterName','killedNum','woundedNum','city']\n",
    "# int2tags=['TAG', 'food', 'adulterant', 'location', 'year']\n",
    "# tags = [0,1,2,3,4]\n",
    "tags = [0,1,2,3]\n",
    "int2tags = \\\n",
    "['TAG',\\\n",
    "'Affected_Food_Product',\\\n",
    "'Produced_Location',\\\n",
    "'Distributed_Location']\n",
    "tags2int = \\\n",
    "{'TAG':0,\\\n",
    "'Affected_Food_Product':1, \\\n",
    "'Produced_Location':2, \\\n",
    "'Distributed_Location':3 }\n",
    "\n",
    "# main loop\n",
    "def main(training_file,trained_model,previous_n,next_n, c, prune):\n",
    "    helper.load_constants()\n",
    "    train_data, identifier = load_data(training_file)\n",
    "\n",
    "    ## extract features\n",
    "    tic = time.clock()\n",
    "    num_words, word_vocab = get_word_vocab(train_data, prune)\n",
    "    \n",
    "    trainX, trainY = get_feature_matrix_n(previous_n,next_n,train_data, num_words, word_vocab, helper.other_features)\n",
    "    print time.clock()-tic\n",
    "\n",
    "    ## train LR\n",
    "    print(\"training\")\n",
    "    tic = time.clock()\n",
    "    clf = LogisticRegression(C=c, multi_class='multinomial', solver='lbfgs')\n",
    "    clf.fit(trainX,trainY)\n",
    "    print time.clock()-tic\n",
    "\n",
    "    feature_list = (word_vocab.keys() + helper.other_features) * (previous_n+next_n+1)  + word_vocab.keys() + ['previous_one'] * len(tags) + ['previous_two'] * len(tags)+ ['previous_three'] * len(tags)\n",
    "    # getTopFeatures(clf,tags,feature_list)\n",
    "    if trained_model != \"\":\n",
    "        pickle.dump([clf, previous_n,next_n, word_vocab,helper.other_features], open( trained_model, \"wb\" ) )\n",
    "    return [clf, previous_n,next_n, word_vocab,helper.other_features]\n",
    "\n",
    "def get_word_vocab(data, prune):\n",
    "    num_words = 0\n",
    "    word_vocab = {}\n",
    "    for sentence in data:\n",
    "        words_in_sentence = set()\n",
    "        for word in sentence[0]:\n",
    "            if word.lower() in words_in_sentence:\n",
    "                continue\n",
    "            if word.lower() not in word_vocab:\n",
    "                word_vocab[word.lower()] = 1\n",
    "            else:\n",
    "                word_vocab[word.lower()] += 1\n",
    "            words_in_sentence.add(word.lower())\n",
    "        num_words += len(sentence[0])\n",
    "    feature_list = []\n",
    "    prune_features(word_vocab,feature_list, prune)\n",
    "    return num_words,word_vocab\n",
    "\n",
    "# reduce dimensions by removing features that don't appear often (not used)\n",
    "def prune_features(feature_vocab, featureList, prune):\n",
    "    for w in feature_vocab.keys():\n",
    "        if feature_vocab[w] <= prune:\n",
    "            feature_vocab.pop(w,None)\n",
    "    index = 0\n",
    "    for w in feature_vocab.keys():\n",
    "        feature_vocab[w] = index\n",
    "        featureList.append(w)\n",
    "        index += 1\n",
    "\n",
    "# get feature matrix given a list of sentences and tags (used for training) n: previous n words\n",
    "def get_feature_matrix_n(previous_n,next_n,data, num_words, word_vocab, other_features,first_n=10):\n",
    "    num_features = len(word_vocab) + len(other_features)\n",
    "    total_features = (previous_n+next_n+1)*num_features + len(word_vocab) + previous_n * len(tags) + first_n\n",
    "    #print num_words, num_features, total_features\n",
    "    dataY = np.zeros(num_words)\n",
    "    dataX = scipy.sparse.lil_matrix((num_words, total_features))\n",
    "    curr_word = 0\n",
    "    for sentence in data:\n",
    "        other_words_lower = set([s.lower for s in sentence[0]])\n",
    "        for i in range(len(sentence[0])):\n",
    "            word = sentence[0][i]\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in word_vocab:\n",
    "                dataX[curr_word,word_vocab[word_lower]] = 1\n",
    "                for j in range(previous_n):\n",
    "                    if i+j+1<len(sentence[0]):\n",
    "                        dataX[curr_word+j+1,(j+1)*num_features+word_vocab[word_lower]] = 1\n",
    "                for j in range(next_n): \n",
    "                    if i-j-1 >= 0:\n",
    "                        dataX[curr_word-j-1,(previous_n+j+1)*num_features+word_vocab[word_lower]] = 1\n",
    "            for (index, feature_func) in enumerate(other_features):\n",
    "                if feature_func(word):\n",
    "                    dataX[curr_word,len(word_vocab)+index] = 1\n",
    "                    for j in range(previous_n):\n",
    "                        if i + j + 1 < len(sentence[0]):\n",
    "                            dataX[curr_word+j+1,(j+1)*num_features+len(word_vocab)+index] = 1\n",
    "                    for j in range(next_n):\n",
    "                        if i - j - 1 >= 0:\n",
    "                            dataX[curr_word-j-1,(previous_n+j+1)*num_features+len(word_vocab)+index] = 1\n",
    "            for other_word_lower in other_words_lower:\n",
    "                if other_word_lower != word_lower and other_word_lower in word_vocab:\n",
    "                    dataX[curr_word,(previous_n+next_n+1)*num_features + word_vocab[other_word_lower]] = 1\n",
    "            for j in range(previous_n):\n",
    "                if j < i:\n",
    "                    dataX[curr_word,(previous_n+next_n+1)*num_features+len(word_vocab)+len(tags) * j + dataY[curr_word-j-1]] = 1\n",
    "            if i < first_n:\n",
    "                dataX[curr_word,(previous_n+next_n+1)*num_features + len(word_vocab) + previous_n * len(tags)+i] = 1\n",
    "            assert len(sentence[0]) == len(sentence[1])\n",
    "            dataY[curr_word] = sentence[1][i]\n",
    "            curr_word += 1\n",
    "    return dataX, dataY\n",
    "\n",
    "# split sentence into a list of words and a list of tags\n",
    "def separate_word_tag(sentence):\n",
    "    parts = sentence.split()\n",
    "    words = []\n",
    "    tags = []\n",
    "    i = 0\n",
    "    for part in parts:\n",
    "        i+=1\n",
    "        #if i > 20:\n",
    "        #    break\n",
    "        tag = \"_\".join(part.split(\"_\")[1:])\n",
    "        try:\n",
    "            tags.append(tags2int[tag])\n",
    "            words.append(part.split(\"_\")[0])\n",
    "        except Exception, e:\n",
    "            pass\n",
    "    return [words,tags]\n",
    "\n",
    "# return a list of raw sentences (unprocessed)\n",
    "def load_data(filename):\n",
    "    sentence_list = [line.rstrip('\\n') for line in open(filename)][1::2]\n",
    "    identifier = [line.rstrip('\\n') for line in open(filename)][::2]\n",
    "    return map(separate_word_tag,sentence_list), identifier\n",
    "\n",
    "# prints a list of top 10 features for each class\n",
    "def getTopFeatures(clf, tags, featureList):\n",
    "    A = np.copy(clf.coef_)\n",
    "    for i in tags:\n",
    "        print int2tags[i]\n",
    "        #A[i] = map(abs, A[i])\n",
    "        indices = np.argsort(A[i])[-20:][::-1]\n",
    "        print indices\n",
    "        for j in indices:\n",
    "            print featureList[j]\n",
    "\n",
    "def save_list_first_names(infile_path,outfile_path):\n",
    "    l = set()\n",
    "    with open(infile_path) as infile:\n",
    "        for line in infile:\n",
    "            l.add(line.split()[0].lower())\n",
    "    print len(l)\n",
    "    pickle.dump(l,open(outfile_path, \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_file = \"../data/tagged_data/EMA/train.tag\" #sys.argv[1]\n",
    "test_file = \"../data/tagged_data/EMA/test.tag\" #sys.argv[1]\n",
    "\n",
    "all_train_data, all_train_identifier = load_data(training_file)\n",
    "\n",
    "# split_index = int( .6 * len(all_train_data))\n",
    "\n",
    "train_data = all_train_data\n",
    "train_identifier = all_train_identifier\n",
    "#dev_data = all_train_data[split_index:]\n",
    "#dev_identifier = all_train_identifier[split_index:]\n",
    "\n",
    "\n",
    "test_data, test_identifier = load_data(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained_model = \"trained_model.EMA.p\" #sys.argv[2]\n",
    "previous_n = 4 #sys.argv[3]\n",
    "next_n = 0\n",
    "c = 4\n",
    "prune = 5\n",
    "\n",
    "# main loop\n",
    "helper.load_constants()\n",
    "all_data = train_data + test_data\n",
    "# train_split = 1\n",
    "# split_index = int(len(all_data)*train_split)\n",
    "# train_data, train_identifier = all_data[:split_index], all_identifier[:split_index]\n",
    "# test_data, test_identifier = all_data[split_index:], all_identifier[split_index:]\n",
    "\n",
    "\n",
    "## extract features\n",
    "tic = time.clock()\n",
    "num_words_train, word_vocab_train = get_word_vocab(train_data, prune)\n",
    "num_words_test, word_vocab_test = get_word_vocab(test_data, prune)\n",
    "num_words_all, word_vocab_all = get_word_vocab(all_data, prune)\n",
    "\n",
    "\n",
    "\n",
    "trainX, trainY = get_feature_matrix_n(previous_n,next_n, train_data, num_words_train, word_vocab_all, helper.other_features)\n",
    "testX, testY = get_feature_matrix_n(previous_n, next_n,   test_data, num_words_test, word_vocab_all, helper.other_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print time.clock()-tic\n",
    "\n",
    "## train LR\n",
    "print(\"training\")\n",
    "tic = time.clock()\n",
    "clf = LogisticRegression(C=c, multi_class='multinomial', solver='lbfgs')\n",
    "clf.fit(trainX,trainY)\n",
    "\n",
    "print time.clock()-tic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141555, 19743)\n",
      "(92622, 19743)\n"
     ]
    }
   ],
   "source": [
    "print trainX.shape\n",
    "print testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8559bd640816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0msplit_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m  \u001b[0mtrainX\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m  \u001b[0mtrainY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'split_index' is not defined"
     ]
    }
   ],
   "source": [
    "assert not  trainX == testX\n",
    "assert not  trainY == testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "ptags = clf.predict(testX)\n",
    "assert not len(test_data) == len(train_data)\n",
    "print len(train_data)\n",
    "print len (test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affected_Food_Product ( 281 , 468 , 631 ) ( 0.600427350427 , 0.445324881141 , 0.511373976342 )\n",
      "Produced_Location ( 75 , 209 , 342 ) ( 0.358851674641 , 0.219298245614 , 0.2722323049 )\n",
      "Distributed_Location ( 73 , 161 , 250 ) ( 0.453416149068 , 0.292 , 0.355231143552 )\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1e79147acaac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# getTopFeatures(clf,tags,feature_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrained_model\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mother_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for ent in tags[1:]:\n",
    "    \n",
    "    correct = [1 if ptags[i] == ent and testY[i] == ent else 0 for i in range(len(ptags))]\n",
    "    guessed = [1 if ptags[i] == ent else 0 for i in range(len(ptags))]\n",
    "    total   = [1 if testY[i] == ent else 0 for i in range(len(ptags))]\n",
    "\n",
    "\n",
    "    accuracy = sum(correct) *1./sum(guessed)\n",
    "    recall   = sum(correct) * 1./sum(total)\n",
    "    f1 = accuracy * recall * 2. / (accuracy + recall)\n",
    "\n",
    "    print int2tags[ent], \"(\" , sum(correct), \",\", sum(guessed), \",\", sum(total), \")\" , \"(\",accuracy, \",\", recall, \",\", f1, \")\"\n",
    "\n",
    "#######\n",
    "# feature_list = (word_vocab.keys() + helper.other_features) * (previous_n+next_n+1)  + word_vocab.keys() + ['previous_one'] * len(tags) + ['previous_two'] * len(tags)+ ['previous_three'] * len(tags)\n",
    "# getTopFeatures(clf,tags,feature_list)\n",
    "if trained_model != \"\":\n",
    "    pickle.dump([clf, previous_n,next_n, word_vocab,helper.other_features], open( trained_model, \"wb\" ) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
