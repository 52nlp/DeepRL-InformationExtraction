{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy.sparse\n",
    "import time\n",
    "import itertools\n",
    "import sys\n",
    "import pickle\n",
    "import helper\n",
    "\n",
    "tags2int = {\"TAG\": 0, \"shooterName\":1, \"killedNum\":2, \"woundedNum\":3, \"city\":4}\n",
    "int2tags = [\"TAG\",'shooterName','killedNum','woundedNum','city']\n",
    "tags = [0,1,2,3,4]\n",
    "\n",
    "\n",
    "def get_word_vocab(data, prune):\n",
    "    num_words = 0\n",
    "    word_vocab = {}\n",
    "    for sentence in data:\n",
    "        words_in_sentence = set()\n",
    "        for word in sentence[0]:\n",
    "            if word.lower() in words_in_sentence:\n",
    "                continue\n",
    "            if word.lower() not in word_vocab:\n",
    "                word_vocab[word.lower()] = 1\n",
    "            else:\n",
    "                word_vocab[word.lower()] += 1\n",
    "            words_in_sentence.add(word.lower())\n",
    "        num_words += len(sentence[0])\n",
    "    feature_list = []\n",
    "    prune_features(word_vocab,feature_list, prune)\n",
    "    return num_words,word_vocab\n",
    "\n",
    "# reduce dimensions by removing features that don't appear often (not used)\n",
    "def prune_features(feature_vocab, featureList, prune):\n",
    "    for w in feature_vocab.keys():\n",
    "        if feature_vocab[w] <= prune:\n",
    "            feature_vocab.pop(w,None)\n",
    "    index = 0\n",
    "    for w in feature_vocab.keys():\n",
    "        feature_vocab[w] = index\n",
    "        featureList.append(w)\n",
    "        index += 1\n",
    "\n",
    "# get feature matrix given a list of sentences and tags (used for training) n: previous n words\n",
    "def get_feature_matrix_n(previous_n,next_n,data, num_words, word_vocab, other_features,first_n=10):\n",
    "    num_features = len(word_vocab) + len(other_features)\n",
    "    total_features = (previous_n+next_n+1)*num_features + len(word_vocab) + previous_n * len(tags) + first_n\n",
    "    #print num_words, num_features, total_features\n",
    "    dataY = np.zeros(num_words)\n",
    "    dataX = scipy.sparse.lil_matrix((num_words, total_features))\n",
    "    curr_word = 0\n",
    "    for sentence in data:\n",
    "        other_words_lower = set([s.lower for s in sentence[0]])\n",
    "        for i in range(len(sentence[0])):\n",
    "            word = sentence[0][i]\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in word_vocab:\n",
    "                dataX[curr_word,word_vocab[word_lower]] = 1\n",
    "                for j in range(previous_n):\n",
    "                    if i+j+1<len(sentence[0]):\n",
    "                        dataX[curr_word+j+1,(j+1)*num_features+word_vocab[word_lower]] = 1\n",
    "                for j in range(next_n): \n",
    "                    if i-j-1 >= 0:\n",
    "                        dataX[curr_word-j-1,(previous_n+j+1)*num_features+word_vocab[word_lower]] = 1\n",
    "            for (index, feature_func) in enumerate(other_features):\n",
    "                if feature_func(word):\n",
    "                    dataX[curr_word,len(word_vocab)+index] = 1\n",
    "                    for j in range(previous_n):\n",
    "                        if i + j + 1 < len(sentence[0]):\n",
    "                            dataX[curr_word+j+1,(j+1)*num_features+len(word_vocab)+index] = 1\n",
    "                    for j in range(next_n):\n",
    "                        if i - j - 1 >= 0:\n",
    "                            dataX[curr_word-j-1,(previous_n+j+1)*num_features+len(word_vocab)+index] = 1\n",
    "            for other_word_lower in other_words_lower:\n",
    "                if other_word_lower != word_lower and other_word_lower in word_vocab:\n",
    "                    dataX[curr_word,(previous_n+next_n+1)*num_features + word_vocab[other_word_lower]] = 1\n",
    "            for j in range(previous_n):\n",
    "                if j < i:\n",
    "                    dataX[curr_word,(previous_n+next_n+1)*num_features+len(word_vocab)+len(tags) * j + dataY[curr_word-j-1]] = 1\n",
    "            if i < first_n:\n",
    "                dataX[curr_word,(previous_n+next_n+1)*num_features + len(word_vocab) + previous_n * len(tags)+i] = 1\n",
    "            dataY[curr_word] = sentence[1][i]\n",
    "            curr_word += 1\n",
    "    return dataX, dataY\n",
    "\n",
    "# split sentence into a list of words and a list of tags\n",
    "def separate_word_tag(sentence):\n",
    "    parts = sentence.split()\n",
    "    words = []\n",
    "    tags = []\n",
    "    i = 0\n",
    "    for part in parts:\n",
    "        i+=1\n",
    "        #if i > 20:\n",
    "        #    break\n",
    "        words.append(part.split(\"_\")[0])\n",
    "        tags.append(tags2int[part.split(\"_\")[1]])\n",
    "    return [words,tags]\n",
    "\n",
    "# return a list of raw sentences (unprocessed)\n",
    "def load_data(filename):\n",
    "    sentence_list = [line.rstrip('\\n') for line in open(filename)][1::2]\n",
    "    identifier = [line.rstrip('\\n') for line in open(filename)][::2]\n",
    "    return map(separate_word_tag,sentence_list), identifier\n",
    "\n",
    "# prints a list of top 10 features for each class\n",
    "def getTopFeatures(clf, tags, featureList):\n",
    "    A = np.copy(clf.coef_)\n",
    "    for i in tags:\n",
    "        print int2tags[i]\n",
    "        #A[i] = map(abs, A[i])\n",
    "        indices = np.argsort(A[i])[-20:][::-1]\n",
    "        print indices\n",
    "        for j in indices:\n",
    "            print featureList[j]\n",
    "\n",
    "def save_list_first_names(infile_path,outfile_path):\n",
    "    l = set()\n",
    "    with open(infile_path) as infile:\n",
    "        for line in infile:\n",
    "            l.add(line.split()[0].lower())\n",
    "    print len(l)\n",
    "    pickle.dump(l,open(outfile_path, \"wb\" ))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_file = \"../data/tagged_data/whole_text_full_city2/train.tag\" #sys.argv[1]\n",
    "\n",
    "all_data, all_identifier = load_data(training_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.284506\n",
      "training\n",
      "5.681214\n"
     ]
    }
   ],
   "source": [
    "trained_model = \"trained_model3.p\" #sys.argv[2]\n",
    "previous_n = 0 #sys.argv[3]\n",
    "next_n = 4\n",
    "c = 10\n",
    "prune = 5\n",
    "\n",
    "# main loop\n",
    "helper.load_constants()\n",
    "all_data, all_identifier = load_data(training_file)\n",
    "train_split = .6\n",
    "split_index = int(len(all_data)*train_split)\n",
    "train_data, train_identifier = all_data[:split_index], all_identifier[:split_index]\n",
    "test_data, test_identifier = all_data[split_index:], all_identifier[split_index:]\n",
    "\n",
    "\n",
    "## extract features\n",
    "tic = time.clock()\n",
    "num_words_train, word_vocab_train = get_word_vocab(train_data, prune)\n",
    "num_words_test, word_vocab_test = get_word_vocab(test_data, prune)\n",
    "num_words_all, word_vocab_all = get_word_vocab(all_data, prune)\n",
    "\n",
    "\n",
    "\n",
    "trainX, trainY = get_feature_matrix_n(previous_n,next_n, train_data, num_words_train, word_vocab_all, helper.other_features)\n",
    "testX, testY = get_feature_matrix_n(previous_n, next_n,   test_data, num_words_test, word_vocab_all, helper.other_features)\n",
    "\n",
    "print time.clock()-tic\n",
    "\n",
    "## train LR\n",
    "print(\"training\")\n",
    "tic = time.clock()\n",
    "clf = LogisticRegression(C=c, multi_class='multinomial', solver='lbfgs')\n",
    "clf.fit(trainX,trainY)\n",
    "\n",
    "print time.clock()-tic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89742, 10485)\n",
      "(57356, 10485)\n"
     ]
    }
   ],
   "source": [
    "print trainX.shape\n",
    "print testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/scipy/sparse/compressed.py:233: SparseEfficiencyWarning: Comparing sparse matrices using == is inefficient, try using != instead.\n",
      "  \" != instead.\", SparseEfficiencyWarning)\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:3: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "print split_index\n",
    "assert not  trainX == testX\n",
    "assert not  trainY == testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "ptags = clf.predict(testX)\n",
    "assert not len(test_data) == len(train_data)\n",
    "print len(train_data)\n",
    "print len (test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent 0\n",
      "len ptags 57356\n",
      "set([0.0, 1.0, 2.0, 3.0, 4.0])\n",
      "FOR  TAG\n",
      "PERCISION 0.988217367423\n",
      "RECALL 0.993816931273\n",
      "F1 0.991009239517\n",
      "-------------\n",
      "NUM CORRECT 57356\n",
      "NUM GUESSED 57356\n",
      "Entities of name 57356\n",
      "_______________________\n",
      "ent 1\n",
      "len ptags 57356\n",
      "set([0.0, 1.0, 2.0, 3.0, 4.0])\n",
      "FOR  shooterName\n",
      "PERCISION 0.288343558282\n",
      "RECALL 0.161512027491\n",
      "F1 0.20704845815\n",
      "-------------\n",
      "NUM CORRECT 57356\n",
      "NUM GUESSED 57356\n",
      "Entities of name 57356\n",
      "_______________________\n",
      "ent 2\n",
      "len ptags 57356\n",
      "set([0.0, 1.0, 2.0, 3.0, 4.0])\n",
      "FOR  killedNum\n",
      "PERCISION 0.84693877551\n",
      "RECALL 0.65873015873\n",
      "F1 0.741071428571\n",
      "-------------\n",
      "NUM CORRECT 57356\n",
      "NUM GUESSED 57356\n",
      "Entities of name 57356\n",
      "_______________________\n",
      "ent 3\n",
      "len ptags 57356\n",
      "set([0.0, 1.0, 2.0, 3.0, 4.0])\n",
      "FOR  woundedNum\n",
      "PERCISION 0.661375661376\n",
      "RECALL 0.581395348837\n",
      "F1 0.618811881188\n",
      "-------------\n",
      "NUM CORRECT 57356\n",
      "NUM GUESSED 57356\n",
      "Entities of name 57356\n",
      "_______________________\n",
      "ent 4\n",
      "len ptags 57356\n",
      "set([0.0, 1.0, 2.0, 3.0, 4.0])\n",
      "FOR  city\n",
      "PERCISION 0.644539614561\n",
      "RECALL 0.499170812604\n",
      "F1 0.56261682243\n",
      "-------------\n",
      "NUM CORRECT 57356\n",
      "NUM GUESSED 57356\n",
      "Entities of name 57356\n",
      "_______________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8b180310e045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#######\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mfeature_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mother_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprevious_n\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnext_n\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mword_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'previous_one'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'previous_two'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'previous_three'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m# getTopFeatures(clf,tags,feature_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrained_model\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for ent in tags:\n",
    "    print \"ent\", ent\n",
    "    print \"len ptags\", len(ptags)\n",
    "    print set(ptags)\n",
    "    correct = [ptags[i] == ent and testY[i] == ent for i in range(len(ptags))]\n",
    "    guessed = [ptags[i] == ent for i in range(len(ptags))]\n",
    "    total   = [testY[i] == ent for i in range(len(ptags))]\n",
    "\n",
    "\n",
    "    accuracy = sum(correct) *1./sum(guessed)\n",
    "    recall   = sum(correct) * 1./sum(total)\n",
    "    f1 = accuracy * recall * 2. / (accuracy + recall)\n",
    "\n",
    "    print \"FOR \", int2tags[ent]\n",
    "    print \"PERCISION\", accuracy\n",
    "    print \"RECALL\", recall\n",
    "    print \"F1\", f1\n",
    "    print \"-------------\"\n",
    "    print \"NUM CORRECT\", len(correct)\n",
    "    print \"NUM GUESSED\", len (guessed)\n",
    "    print \"Entities of name\", len(total)\n",
    "    print \"_______________________\"\n",
    "\n",
    "#######\n",
    "feature_list = (word_vocab.keys() + helper.other_features) * (previous_n+next_n+1)  + word_vocab.keys() + ['previous_one'] * len(tags) + ['previous_two'] * len(tags)+ ['previous_three'] * len(tags)\n",
    "# getTopFeatures(clf,tags,feature_list)\n",
    "if trained_model != \"\":\n",
    "    pickle.dump([clf, previous_n,next_n, word_vocab,helper.other_features], open( trained_model, \"wb\" ) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
